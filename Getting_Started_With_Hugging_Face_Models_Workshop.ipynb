{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤— Getting Started With Hugging Face Models ðŸ¤—\n",
        "\n",
        "Hey! Welcome to this workshop!\n",
        "\n",
        "For starters, please enjoy some ASCII art ðŸŽ¨"
      ],
      "metadata": {
        "id": "7kPxI8AngwOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "                                        ........                          \n",
        "                                  .:-==============-:.                    \n",
        "                               :====----------------====:.                \n",
        "                             :===------------------------===:              \n",
        "                           .==------------------------------==.            \n",
        "                          -==--------------------------------==-           \n",
        "                        .==------------------------------------==.         \n",
        "                        ==--------=*##*=---------=*###+---------==         \n",
        "                       ==-----==--######---------*#####+---=-----==        \n",
        "                      :=-----====-+#*===---------=+=+#*=-=====----=:       \n",
        "                      ==-----===--------------------------===-----==       \n",
        "                      ==------------------------------------------==       \n",
        "                      ==------------=#*++=------=+*#*-------------==       \n",
        "                      ==-------------*#####***######+-------------==       \n",
        "                      :=====---------=#####*##*####*----------=====:       \n",
        "                        =======-=====--=+**********+---=====-=======        \n",
        "                    :--===---====--==----===++===----===--===---===---     \n",
        "                    :==--====--===--===--------------===--===--=====--=-    \n",
        "                    .====---===--===--==------------===--===-====--====:    \n",
        "                  .========--==--=----===--------===----=--==--========.   \n",
        "                    ====---===-----------==------===----------====--====    \n",
        "                    ========-------------===----===-------------=====-==.   \n",
        "                    :====--=------------============----------------===-    \n",
        "                      .:-======-------===-:::::::::====------======-:.      \n",
        "                          .::---===---:            .:--===---::.\n",
        "                                          _                 ___              \n",
        "                  /\\  /\\_   _  __ _  __ _(_)_ __   __ _    / __\\_ _  ___ ___\n",
        "                / /_/ / | | |/ _` |/ _` | | '_ \\ / _` |  / _\\/ _` |/ __/ _ \\\n",
        "                / __  /| |_| | (_| | (_| | | | | | (_| | / / | (_| | (_|  __/\n",
        "                \\/ /_/  \\__,_|\\__, |\\__, |_|_| |_|\\__, | \\/   \\__,_|\\___\\___|\n",
        "                              |___/ |___/         |___/                      \n",
        "\t\t           __      __             __           .__                                           \n",
        "\t\t          /  \\    /  \\___________|  | __  _____|  |__   ____ ______                          \n",
        "\t\t          \\   \\/\\/   /  _ \\_  __ \\  |/ / /  ___/  |  \\ /  _ \\\\____ \\                         \n",
        "\t\t           \\        (  <_> )  | \\/    <  \\___ \\|   Y  (  <_> )  |_> >                        \n",
        "\t\t            \\__/\\  / \\____/|__|  |__|_ \\/____  >___|  /\\____/|   __/                         \n",
        "\t\t                 \\/                   \\/     \\/     \\/       |__|                          \n",
        "                                     __  `               \n",
        "\t\t                            /\\ \\                 \n",
        "\t\t                            \\ \\ \\____  __  __    \n",
        "\t\t                             \\ \\ '__`\\/\\ \\/\\ \\   \n",
        "\t\t                              \\ \\ \\L\\ \\ \\ \\_\\ \\  \n",
        "\t\t                               \\ \\_,__/\\/`____ \\\n",
        "\t\t                                \\/___/  `/___/> \\\n",
        "\t\t                                           /\\___/\n",
        "\t\t                                           \\/__/\n",
        "\t\t              ('-.         .-') _                .-')               ('-. .-.\n",
        "\t\t             ( OO ).-.    ( OO ) )              ( OO ).            ( OO )  /\n",
        "\t\t       ,--.  / . --. /,--./ ,--,'  .-'),-----. (_)---\\_)   .-----. ,--. ,--.\n",
        "\t\t   .-')| ,|  | \\-.  \\ |   \\ |  |\\ ( OO'  .-.  '/    _ |   '  .--./ |  | |  |\n",
        "\t\t  ( OO |(_|.-'-'  |  ||    \\|  | )/   |  | |  |\\  :` `.   |  |('-. |   .|  |\n",
        "\t\t  | `-'|  | \\| |_.'  ||  .     |/ \\_) |  |\\|  | '..`''.) /_) |OO  )|       |\n",
        "\t\t  ,--. |  |  |  .-.  ||  |\\    |    \\ |  | |  |.-._)   \\ ||  |`-'| |  .-.  |\n",
        "\t\t  |  '-'  /  |  | |  ||  | \\   |     `'  '-'  '\\       /(_'  '--'\\ |  | |  |\n",
        "\t\t   `-----'   `--' `--'`--'  `--'       `-----'  `-----'    `-----' `--' `--'\n",
        "\n",
        "  ```"
      ],
      "metadata": {
        "id": "TuVy73aAhPLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro to Hugging Face and Website Tour\n",
        "\n",
        "- Duration: About 15 min\n",
        "- Content: Introductory talk to Hugging Face to answer the question: \"What is Hugging Face, what can it do for you and why would you use it?\n",
        "- Goal: Get an overview about the Hugging Face's most important features.\n",
        "\n",
        "Please just lean back and enjoy the show ðŸ¤—\n",
        "\n",
        "For those of you who complete this notebook asynchonously, please refer to the recording for this tour (https://bln.techlabs.org/event/87570bd8-d573-40ec-a9c9-fea80776748c) or even better just explore it yourself. There is so much to discover! In case you want to dig deeper, the Hugging Face Docs are always a great place to look for answers (https://huggingface.co/docs).\n",
        "\n"
      ],
      "metadata": {
        "id": "UPOjjwnKmzfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Model Cards\n",
        "\n",
        "### Overview\n",
        "- Duration: 5-10 min\n",
        "- Content: Browse GPT-2's model card in the Hugging Face Model Hub and have a look at uses, risks, biases and technical specifications.\n",
        "- Goal: Familiarize with model cards.\n",
        "\n",
        "### Description\n",
        "If today you were to tell me you have never heard about GPT, I would hardy even believe you. Since its release in December 2022, Chat-GPT took everyones attention by storm and is all over the media. As of August 2023, the free version builds uses GPT version 3.5, while paid users can access GPT-4. On Hugging Face, we have open source access to one of its latest predecessors: GPT-2 XL. Now that you had an intro to the Model Hub, let's dive in a little deeper and answer some specific questions about GPT-2 XL. Being able to extract knowledge and information from the Model Hub is crucial if you want to start using them in your own projects!\n",
        "\n",
        "\n",
        "Please find GPT-2 XL in Hugging Face's Model Hub following this link:\n",
        "https://huggingface.co/gpt2-xl\n",
        "\n",
        "\n",
        "#### Easy\n",
        "- How does GPT-2 XL differ from other GPT-2 versions?\n",
        "- Which deep learning architecture is it based on?\n",
        "- What is the vocabulary size?\n",
        "- How many parameters does this version have?\n",
        "- Which frameworks is it available in?\n",
        "\n",
        "#### Intermediate\n",
        "- What is the activation function?\n",
        "- When was the last time the repository has been updated? What changes have been made?\n",
        "- What is the file size of the PyTorch model?\n",
        "\n",
        "#### Advanced\n",
        "- What is a token? How many tokens can it put out?\n",
        "\n",
        "\\\n",
        "\n",
        "**Hint:** Some of the info can be found in the model card (or rather `README.md` file), while most can be found in the `config.json` file. For one question you need to have a look at the repo itself."
      ],
      "metadata": {
        "id": "xdsF26WsD0mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Deploy your own Model\n",
        "\n",
        "- Duration: 15-20 min\n",
        "- Content: Training a micro-model in Scikit-learn, initializing a Hugging Face Git Repo, deploying the model there, add and refine the model card\n",
        "- Goal: Once you are ready to train your own models, you might want to upload them to Hugging Face. Here you will learn how to do that.\n",
        "\n",
        "### Adding a Model to the Model Hub\n",
        "\n",
        "Letâ€™s say you want to upload a model to the Hub. This model could be a model of any ML library: Scikit-learn, Keras, Transformers, etc.\n",
        "\n",
        "Letâ€™s go through the steps:\n",
        "\n",
        "1. Create a Hugging Face account. Then go to [huggingface.co/new](http://huggingface.co/new) to create a new model repository. The repositories you make can be either public or private. I recomment to just make it private, but really it does not matter. Add a license. For this demo, just enter `cc` to set up a creative commons license. This will also automatically create a README.md file for you and adds a section about metadata (right now just the licence). The README functions as your model card, and we can add tags to the metadata later on.\n",
        "\n",
        "2. You start with a repo that has a model card. You can upload your model either by using the Web UI or by doing it with Git. If youâ€™ve never used Git before, we suggest just using the Web interface. You can click `Add File` and drag and drop the files you want to add. If you have Git installed on your system and want to understand the complete workflow, letâ€™s go with the Git approach.\n",
        "\n",
        "    1. Install both git and git-lfs installed on your system.\n",
        "        1. Git: [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
        "        2. Git-lfs: [https://git-lfs.github.com/](https://git-lfs.github.com/). Large files need to be uploaded with Git LFS.  Git does not work well once your files are above a few megabytes, which is frequent in ML. ML models can be up to gigabytes or terabytes! ðŸ¤¯\n",
        "    2. Clone the repository you just created\n",
        "\n",
        "        ```\n",
        "        git clone https://huggingface.co/<your-username>/<your-model-id>\n",
        "        ```\n",
        "\n",
        "    3. Go to the directory and initialize Git LFS\n",
        "        1. Optional and most likely not needed for you except you have already trained your own fancy model in a rare framework: Hugging Face already provides a list of common file extensions for the large files in `.gitattributes`, If the files you want to upload are not included in the `.gitattributes` file, you might need as shown here: You can do so with\n",
        "\n",
        "            ```\n",
        "            git lfs track \"*.your_extension\"\n",
        "            ```\n",
        "        2. Initialize the Git LFS using this command:\n",
        "\n",
        "            ```\n",
        "             git lfs install\n",
        "            ```\n",
        "\n",
        "    4. Add your files to the repository. For this, you can just copy paste or drag and drop them there. If you feel adventurous, you can also try using the command line. The files depend on the framework/libraries youâ€™re using. Overall, what is important is that you provide all artifacts required to load the model. For example:\n",
        "        1. For TensorFlow, you might want to upload a SavedModel or `h5` file.\n",
        "        2. For PyTorch, usually, itâ€™s a `pytorch_model.bin`.\n",
        "        3. For Scikit-Learn, itâ€™s usually a `joblib` file.\n",
        "\n",
        "    Most of you probably do not yet have their own trained model files. But don't worry! This is why we are here today! We can just use Scikit-learn to train a very simple machine learning model in absolutely not time. To do that, just run the code cells below. In each cell, comments explain what the code does. If you happen to have your own trained model or some other model file, feel free to use these instead.\n",
        "\n",
        "    Disclaimer: To run this in Google Colab, you need a Google account for signing in. I assume that each of you has one. If you don't, please either:\n",
        "     1. Create one now (recommended)\n",
        "     2. Run the Notebook locally (if you know how to do that and have all the dependencies installed)\n",
        "     3. Feel free to follow the workshop as a spectator (not recommended)"
      ],
      "metadata": {
        "id": "dZCHOlOpmgdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting the drive allows you to use your Google Drive hard drive\n",
        "\n",
        "# connect colab to google drive by mounting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "v2Pln2PdEPM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a very simple machine learning model for you. It is not actually too important what the model does right now. This exercise is about adding your own model to the Model Hub and small files are advantageous for this. In my case, it has just 568 Bytes, which is extremely small for a typical model uploaded to Hugging Face. This does not even need the large file storage extension, but since it is not a typical model and usually you would upload way larger files, we will still follow best the practices."
      ],
      "metadata": {
        "id": "kmcPko5xqRV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the linear model functionality from the Scikit-learn package\n",
        "from sklearn import linear_model\n",
        "# initialize a linear model\n",
        "reg = linear_model.LinearRegression()\n",
        "# fit the model to the data\n",
        "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "\n",
        "# import functionalities for saving and loading models from the joblib package\n",
        "from joblib import dump, load\n",
        "# save the model (it is a REGression model) under file name \"model.joblib\"\n",
        "dump(reg, 'model.joblib')"
      ],
      "metadata": {
        "id": "e5TdVv-kD7YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  5. Once the cell finished executing, please download the model file. In the top left corner, below the Colab logo, you can find a symbol for files. Click that! Your model file, called \"model.joblib\" will appear among your files (most likely at the very bottom). Right click, then download the file. Now move the downloaded model file to your Hugging Face repo that you just cloned to your local computer. Commit and push your files (make sure the saved file is within the repository). Do not run the comments (to be recognized by the hashtag symbol #). They are just for explaining the steps of the Git workflow.\n",
        "\n",
        "    ```python\n",
        "    git status # check if everything goes as planned. git should have noticed the new file\n",
        "    git add . # stage the file for being committed\n",
        "    git commit -m \"First model version\" # commit with a message. You can write something else if you like\n",
        "    git push # push to remote\n",
        "    ```\n",
        "\n",
        "Disclaimer: Pushing like this does work and is sufficient for this workshop. BUT for future projects, please set up authentification via SSH. This will provide appropriate security and Hugging Face will soon deprecate other ways of authentification.\n",
        "\n",
        "And we're done! You can check your repository with all the recently added files!\n",
        "\n",
        "The UI allows you to explore the model files and commits and to see the diff introduced by each commit. Now that the model is in the Hub, others can find them! You can also collaborate with others easily by creating an organization. Hosting through the Hub allows a team to update repositories and do things you might be used to, such as working in branches and working collaboratively. The Hub also enables versioning in your models: if a model checkpoint is suddenly broken, you can always head back to a previous version.\n",
        "\n",
        "At the top of the `README`, you can find some metadata. You will only find the license right now, but you can add more things. Letâ€™s try some of it:\n",
        "\n",
        "```yaml\n",
        " tags:\n",
        "- tabular #Â the tags will automatically be detected as task tags.\n",
        "- regression\n",
        "- tabular-regression # you can get even more specific\n",
        "datasets:\n",
        "- custom # This will link to a dataset on the Hub if it exists. Right now just a place holder.\n",
        "```\n",
        "\n",
        "**Challenge 5**. Using the [documentation](https://huggingface.co/docs/hub/model-repos#how-are-model-tags-determined), change the default example in the widget.\n",
        "\n",
        "The metadata allows people to discover your model quickly. Your model will now show up when you search for tabular, regression or tabular-regression. Had we used a real data set, he model would also show up when looking at the dataset.\n",
        "\n",
        "**Bonus task for motivated Techies and AI enthusiasts:** Browse the data sets and find one that would be suitable for training a tabular-regression algorithm like ours. Think of a cool project idea and ways to use it.\n",
        "\n",
        "Exercise 1 is based on Hugging Face's Education Toolkit: https://github.com/huggingface/education-toolkit/blob/main/01_huggingface-hub-tour.md"
      ],
      "metadata": {
        "id": "Mh_doUpSUEhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Accessing Models from the Model Hub\n",
        "\n",
        "- Duration: About 20 min\n",
        "- Content: Demo of stable-diffusion, use pipelines and frameworks to access GPT-2 or another model of your choice\n",
        "- Goal: See how you can use most of Hugging Face's models, have fun with Stable Diffusion\n",
        "\n",
        "## Stable Diffusion Image Generating AI\n",
        "\n",
        "Find the Model Card following this link:\n",
        "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n",
        "\n",
        "### Disclaimers\n",
        "\n",
        "1. Sometimes after you just installed multiple packages, loading them is not possible immediately and you run into errors. Most of the time this can be solved by just restarting the runtime. Then, just re-run the all steps **except for the installing**. Omit installing again! Usually you are able to run the code then.\n",
        "\n",
        "2. This is code is **truly greedy for memory**. If you are running this on Googla Colab, you are using a free service that comes with its limits. The free version only has 15 GB of GPU-RAM. RAM usage is displayed in the top right corner. Click on it to expand. If you run out of memory, the solution again is to restart the runtime and re-run the desired code cells.\n",
        "\n",
        "### Preparations\n",
        "Run the code cells below to set everything up."
      ],
      "metadata": {
        "id": "_xJjLrw6xryZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you need to restart runtime, use this to mount again\n",
        "\n",
        "# connect colab to google drive by mounting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J0DysCmXzbZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usually, it should not be necessary to install this. If it does not work without, install it.\n",
        "# for installation, activate the last two lines\n",
        "# only run this ONCE! if you need to restart runtime, do not run it again\n",
        "\n",
        "# install and update requirements for running stable diffusion\n",
        "# the flag --quiet prevents the cell to output all the logs keeping the notebook short\n",
        "#!pip install --quiet diffusers --upgrade\n",
        "#!pip install --quiet invisible_watermark transformers accelerate safetensors"
      ],
      "metadata": {
        "id": "zs171WU8x01G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the requirements\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# create a pipe and direct it to cuda (parallel computing platform for rapid progressing)\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "O-cF1GFzyDNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PrOoOoMpT eNgInEeRiNg\n",
        "Enter your promt here to tell the model what you want it to generate.\n",
        "\n",
        "Feel free to experiment with it. Since I am a huge fan of Tolkien and The Lord of the Rings, I decided to let it draw Gandalf. Get creative!\n",
        "\n",
        "If you run out of memory, just restart the runtime."
      ],
      "metadata": {
        "id": "DbcSrjL_7guB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter your PrOoMpT here and become a true pRoOmTiNg EnGiNeEr\n",
        "prompt = \"Gandalf the Grey sitting in a light Greenhouse on a sunny day laughing and blowing colorful smoke rings from his pipe into the air photorealistic\"\n",
        "\n",
        "# call stable diffusion, assign image to an object\n",
        "# (be careful, it is not saved yet! If you want to keep the images, you need to save them later on!)\n",
        "image = pipe(prompt=prompt).images[0]"
      ],
      "metadata": {
        "id": "8ctWe80Q38Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the image\n",
        "The image is saved in an object called image `image` (surprise!). Be careful, because once you run a new prompt, this image will be overwritten.\n",
        "\n",
        "There are options to generate and display multiple images at once. A more beginner friendly option to keep your pictures displayed in this notebook is to just create a new cell and display the second image there. Finally, you could off course also rename the object each time, but I would not consider this good practice.\n"
      ],
      "metadata": {
        "id": "CAXyXz_68F7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to display the image\n",
        "image"
      ],
      "metadata": {
        "id": "0R2CNALp2Xvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the second image if you generated more than one\n",
        "# if you only generated one, there is no use in running this cell\n",
        "# add more cells like this for more images\n",
        "image"
      ],
      "metadata": {
        "id": "Pbw0PixByDI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save your images\n",
        "In case you really like the image, you can save it. If you do not save it, it will be lost. In my opition, not every image is worth saving though. As always, there are multiple options for this:\n",
        "\n",
        "1. You can simply right click and save it to your local system.\n",
        "2. Run the code below to save it in a decicated folder in your Google Drive. It might take a few minutes to appear there. **Rename your images!** I made sure the old ones will not be overwritten, but the new ones will not be saved if you do not change the file's name.\n",
        "\n",
        "Check if it worked:\n",
        "https://drive.google.com/drive/my-drive"
      ],
      "metadata": {
        "id": "04HAIYmT9nAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import python operating system library\n",
        "import os\n",
        "\n",
        "# define path to save images at\n",
        "path = 'drive/MyDrive/stable_diffusion_techlabs'\n",
        "\n",
        "# make a new folder for saving your images\n",
        "# if folder already exists, it will not make it again\n",
        "if not os.path.exists(path):\n",
        "   os.makedirs(path)\n",
        "\n",
        "# SET NAME FOR IMAGE HERE\n",
        "image_name = \"gandalf6.png\"\n",
        "\n",
        "# join path and image name\n",
        "image_path = os.path.join(path, image_name)\n",
        "\n",
        "# save the image to your Google Drive\n",
        "# if image already exists, it will not overwrite it\n",
        "# instead, you will be asked to choose a different name\n",
        "if not os.path.exists(image_path):\n",
        "  image.save(image_path)\n",
        "  print(\"Image saved.\")\n",
        "else: print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n File already exists. Rename image, please. \\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
      ],
      "metadata": {
        "id": "ZeQcvKPTyDA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus Task for **after** the workshop\n",
        "There are tons of options to customize stable-diffision's image generating. To get you started, you can:\n",
        "- Generate multiple images at once using a list of prompts\n",
        "- We were running the base model. You can run the whole base + refiner pipeline as an ensemble of experts.\n",
        "- Customize, for exampe, the number of steps, the random seed and much more\n",
        "- Use different versions and releases\n",
        "- You could even use a completely different model after all\n",
        "\n",
        "If you feel like it, try out to implement some of these.\n",
        "\n",
        "@All all the Techies: I would love to see your improvements to my code in the Community Channel! :)\n",
        "\n",
        "There is much more to discover! I hope that now you know where to find all the relevant information. Use model cards, the Hugging Face docs and forums/communities, YouTube tutorials, Stack Overflow, ChatGPT, blog posts in Medium etc. to find out about all the possibilities and get help if you are stuck.\n",
        "\n",
        "## Your turn: Let's get back to GPT-2\n",
        "\n",
        "Because of memory, we will use a slightly smaller version of GPT-2, GPT-2 Large, this time. Your task is to access the model.\n",
        "\n",
        "You can find its model card following this link: https://huggingface.co/gpt2-large\n",
        "\n",
        "\n",
        "GPT-2 is a transformers based model. For it we can make use of pipelines. This is a feature that abstracts a lot of code for you and sets the system up for you. It is kind of the plug and play version. It uses the defaults, however, and is not customizable too much. This is cool if you just want to use it quickly and are fine with the defaults.\n",
        "\n",
        "#### Easy\n",
        "Recommended: Try this first.\n",
        "\n",
        "Use a pipeline to access GPT-2 Large.\n",
        "\n",
        "#### Advanced\n",
        "If you are bored already, you can use a framework. You will need to make some adaptations to the provided code, however. Alternatively choose any other model from the Model Hub and try to make it work. If you have a super cool model running, you can share your screen later on if you want.\n",
        "\n"
      ],
      "metadata": {
        "id": "VTKGuWnTCILN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "55MvPMo8cAhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Gradio and Hugging Face Spaces\n",
        "\n",
        "- Duration: About 15 min\n",
        "- Content: Demo of Gradio and Hugging Face Spaces, how to build a Gradio interface\n",
        "- Goal: Enable you to use these technologies and apply them to your own models, get confident enough to Hugging Face Spaces yourself and be able to use some cool Spaces\n",
        "\n",
        "### Building Gradio Demos in Python\n",
        "\n",
        "Demos allow machine learning developers to showcase their work easily. They help to provide the models to a wider and more diverse audience, which contributes to identify bugs. Gradio is a Python library for building web demos. It can automatically recognize input and output based on the provided model, but can be highly customized, provided the model's requirements are met. It allows for providing examples to help users from different backgrounds getting started with using the model.\n",
        "\n",
        "Below, an example setting up a simple Gradio interface for GPT-2 Large is displayed."
      ],
      "metadata": {
        "id": "cySkMwmMk2Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the Gradio library\n",
        "!pip install --quiet gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "RU8jDmD7jEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set examples for the user to click on\n",
        "examples = [[\"One Ring to rule them all.\"], [\"I hope you enjoy the Hugging Face workshop!\"]]"
      ],
      "metadata": {
        "id": "OciEDLPEj3KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launch gradio\n",
        "# choose a model, set examples, launch\n",
        "gr.Interface.load(\"huggingface/gpt2-large\", inputs=\"text\", outputs=\"text\", examples=examples).launch();"
      ],
      "metadata": {
        "id": "VFrxUgHbjc8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some cool Hugging Face Spaces\n",
        "The following Spaces are hosted already and loaded"
      ],
      "metadata": {
        "id": "hRYpCoz8x1nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import display for showing hosted Hugging Face Spaces in Google Colab\n",
        "from IPython.display import IFrame"
      ],
      "metadata": {
        "id": "ljGbvFwH2nUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my image classifier for last semester's TechLabs project \"WasteWise\"\n",
        "IFrame(src='https://hf.space/gradioiframe/fabianjkrueger/WasteWise_API/+', width=1000, height=800)"
      ],
      "metadata": {
        "id": "kJumo_bQx9WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat with Llama2\n",
        "IFrame(src='https://hf.space/gradioiframe/chansung/llama2-with-gradio-chat', width=1000, height=800)"
      ],
      "metadata": {
        "id": "w0Ov4N800rM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another text image generator as well as image to image generator\n",
        "IFrame(src='https://hf.space/gradioiframe/multimodalart/LoraTheExplorer', width=1000, height=800)"
      ],
      "metadata": {
        "id": "-hiUFzQNwNAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- My webcam WasteWise Gradio App using the WebCam: https://huggingface.co/spaces/fabianjkrueger/WasteWise (Goes to sleep after 48 hours of inactivity. If you complete this asynchronously, you might need to restart the Space if you care to see it.)\n",
        "\n",
        "### Your turn: Build your first Gradio Demo\n",
        "\n",
        "Use any model from the Model Hub to build a Gradio interface. You can browse for tasks to get inspired. Unfortunately, some models require a key. This is the case for almost all the text-to-video models.\n",
        "\n",
        "If you cannot find one that works, you can use GPT-2 and expand the interface. Also, here are two others that work nicely:\n",
        "https://huggingface.co/facebook/fastspeech2-en-ljspeech\n",
        "https://huggingface.co/microsoft/git-large-coco\n",
        "\n",
        "Try to set up some examples or customize the output.\n",
        "\n",
        "Here is the Gradio Quickstart: https://www.gradio.app/guides/quickstart"
      ],
      "metadata": {
        "id": "OYSB3apWo1Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "c61GQFEN7WPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy and Host Gradio Demos in Hugging Face\n",
        "\n",
        "You can host Gradio demos in Hugging Face for free. Beyond that, you can embed Gradio demos on any website. One of the coolest things is that you can access the models using an API that can be used in Python, JavaScript and Curl. This comes in extremely handy if you plan to integrate it into a web or mobile app. At TechLabs, where you work in interdisciplinary teams, this is a great opportunity for Artificial Intelligence or Data Science Techies techies to provide their model to the Web Development techies. There is a button at the bottom that says \"Use via API\". If you follow it, you get all the instructions you need.\n",
        "\n",
        "Here are two tutorials for hosting your Gradio apps in Hugging Face Spaces:\n",
        "- Hugging Face Gradio Spaces: https://huggingface.co/docs/hub/spaces-sdks-gradio\n",
        "- Gradio using Hugging Face Integrations: https://gradio.app/guides/using-hugging-face-integrations\n",
        "\n",
        "\n",
        "Once you have your own trained models and want to deploy them, you might want to consider this option. Off course, there are a lot of alternative ways out there. All of them come with pros and cons, but I personally think that this is a very nice one.\n",
        "\n",
        "# ðŸ¤— I hope you had a great workshop and to see you again! ðŸ¤—\n",
        "\n",
        "**If you have any questions, I will now do my best to answer them!**\n",
        "\n",
        "Finally, some closing words. I really do think that helping others in today's world has become more important than ever. At the same time, you can contribute to make someone's day just a little bit better and get so much joy from the deed yourself. I really hope that you could find some value in this and that I was able to motivate you to try out some of the amazing things offered by Hugging Face! Now it's your turn. Go out and help others on their mission to become better programmers. Contribute to Open Source, write blog posts, engage in forums and discussions, teach what you can and spread the word <3\n",
        "\n",
        "#### Feel free to add or follow me on:\n",
        "LinkedIn: https://www.linkedin.com/in/fabianjkrueger/ \\\n",
        "X/Twitter: https://twitter.com/fabianjkrueger \\\n",
        "GitHub: https://github.com/fabianjkrueger \\\n",
        "Or reach out via Slack or e-mail: janosch.krueger@techlabs.org"
      ],
      "metadata": {
        "id": "OOtZ202oMATG"
      }
    }
  ]
}